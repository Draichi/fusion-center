# Project Overwatch - Environment Variables
# Copy this file to .env and fill in your API keys

# =============================================================================
# MCP Server Configuration
# =============================================================================

MCP_SERVER_NAME=project-overwatch
MCP_SERVER_HOST=127.0.0.1
MCP_SERVER_PORT=8080

# =============================================================================
# Data Source API Keys
# =============================================================================

# NASA FIRMS - Required for thermal anomaly detection
# Get your free key at: https://firms.modaps.eosdis.nasa.gov/api/area/
NASA_FIRMS_API_KEY=

# Cloudflare API (optional) - For detailed traffic metrics
# Get at: https://dash.cloudflare.com/profile/api-tokens
CLOUDFLARE_API_TOKEN=

# Telegram API - Required for OSINT channel monitoring
# Get your credentials at: https://my.telegram.org
# 1. Log in with your phone number
# 2. Go to "API development tools"
# 3. Create a new application
TELEGRAM_API_ID=
TELEGRAM_API_HASH=
# After setting credentials, run once: python scripts/telegram_auth.py

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# Google Gemini
# Get at: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=

# Anthropic Claude
# Get at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# xAI Grok
# Get at: https://console.x.ai/
XAI_API_KEY=

# Ollama (local) - No key needed, just the URL
OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# Agent Configuration
# =============================================================================

# Default provider: gemini, anthropic, grok, ollama
AGENT_PROVIDER=gemini

# Model names per provider (customize as needed)
AGENT_MODEL_GEMINI=gemini-2.0-flash-exp
AGENT_MODEL_ANTHROPIC=claude-3-5-sonnet-20241022
AGENT_MODEL_GROK=grok-beta
AGENT_MODEL_OLLAMA=llama3.2

# Agent behavior
AGENT_TEMPERATURE=0.7
AGENT_MAX_ITERATIONS=10

# =============================================================================
# Dual LLM Configuration
# =============================================================================

# Structured Output LLM (JSON mode, for planning/analysis/correlation nodes)
STRUCTURED_LLM_PROVIDER=ollama
STRUCTURED_LLM_MODEL=qwen2.5:7b
STRUCTURED_LLM_TEMPERATURE=0.3

# Thinking/Reasoning LLM (for reflection/verification/synthesis nodes)
THINKING_LLM_PROVIDER=ollama
THINKING_LLM_MODEL=llama3.2
THINKING_LLM_TEMPERATURE=0.7

# =============================================================================
# Logging
# =============================================================================

LOG_LEVEL=INFO

# =============================================================================
# Agent v2 (PydanticAI) Configuration
# =============================================================================

# Debug prompts - logs all prompts to logs/prompts_<timestamp>/ directory
# Set to 1, true, or yes to enable
DEBUG_PROMPTS=0

OLLAMA_BASE_URL=http://localhost:11434/v1
